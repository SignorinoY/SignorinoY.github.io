<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gong Ziyang</title>
  <icon>https://gongziyang.com/icon.png</icon>
  
  <link href="https://gongziyang.com/atom.xml" rel="self"/>
  
  <link href="https://gongziyang.com/"/>
  <updated>2024-06-26T07:22:41.407Z</updated>
  <id>https://gongziyang.com/</id>
  
  <author>
    <name>Gong Ziyang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Experiment Report: BERT Pre-trained Model</title>
    <link href="https://gongziyang.com/post/bert-pre-training-model-experimental-report/"/>
    <id>https://gongziyang.com/post/bert-pre-training-model-experimental-report/</id>
    <published>2024-06-10T10:00:00.000Z</published>
    <updated>2024-06-26T07:22:41.407Z</updated>
    
    <content type="html"><![CDATA[<p>This experiment report explores the process of using the open-sourceBERT (Bidirectional Encoder Representations from Transformers) model forChinese text classification. By fine-tuning this model on a specificnews dataset, we evaluate its classification performance and accuracy.The report systematically analyzes the basic principles, architecturaldesign, pre-training tasks, and fine-tuning methods of the BERTpre-trained model while providing experimental results to assess themodel's performance.</p><span id="more"></span><h1 id="model-architecture">Model Architecture</h1><p>BERT is a language model based on Transformers proposed by the GoogleAI team in 2018, which significantly improved the performance of variousnatural language processing tasks through pre-training and fine-tuning<span class="citation" data-cites="devlin2019bert">(<ahref="#ref-devlin2019bert" role="doc-biblioref">Devlin et al.,2019</a>)</span>. The architecture of the BERT model consists ofmultiple layers of self-attention and feedforward neural network layers.In this experiment, we adopted the BERT-base model architecture, whichincludes 12 layers, 768 hidden units, 12 attention heads, and a total of110 million parameters. It is noteworthy that the BERT-base model issimilar to the architecture of OpenAI's GPT model but uses bidirectionallanguage model objectives during training.</p><h1 id="pre-training-tasks">Pre-training Tasks</h1><p>To better perform Chinese natural language processing tasks, weselected the following BERT pre-trained models for this experiment, withspecific information as follows:</p><table><colgroup><col style="width: 34%" /><col style="width: 14%" /><col style="width: 19%" /><col style="width: 31%" /></colgroup><thead><tr><th style="text-align: center;">Model</th><th style="text-align: center;">Dataset</th><th style="text-align: center;">Mask</th><th style="text-align: center;">Pre-training Tasks</th></tr></thead><tbody><tr><td style="text-align: center;"><code>BERT</code> <span class="citation"data-cites="devlin2019bert">(<a href="#ref-devlin2019bert"role="doc-biblioref">Devlin et al., 2019</a>)</span></td><td style="text-align: center;">wiki</td><td style="text-align: center;">Word Piece</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>BERT-wwm</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki</td><td style="text-align: center;">WWM</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>BERT-wwm-ext</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki+ext</td><td style="text-align: center;">WWM</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>RoBERTa-wwm-ext</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki+ext</td><td style="text-align: center;">WWM</td><td style="text-align: center;">DMLM</td></tr></tbody></table><ul><li><strong>Dataset</strong>: <code>wiki</code> represents ChineseWikipedia, <code>ext</code> represents other encyclopedias, news, QAdatasets, etc.</li><li><strong>Mask</strong>: <code>Word Piece</code> indicates the use ofsubword tokenization, ignoring traditional Chinese word segmentation(CWS); <code>WWM</code> (Whole Word Masking) uses the LTP tool from HITto mask all characters in the same word. <span class="citation"data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></li><li><strong>Pre-training Tasks</strong>: <code>MLM</code> (MaskedLanguage Model) randomly masks some tokens in the input and thenpredicts these masked tokens; <code>NSP</code> (Next SentencePrediction) predicts whether two sentences are consecutive;<code>DMLM</code> (Dynamic Masking Language Model) randomly masks sometokens in the input and dynamically adjusts the masking ratio.</li></ul><h1 id="fine-tuning-tasks">Fine-tuning Tasks</h1><p>In the downstream task of text classification in this experiment, weadopted the method proposed by <span class="citation"data-cites="devlin2019bert">Devlin et al. (<a href="#ref-devlin2019bert"role="doc-biblioref">2019</a>)</span>. We extracted the output vector<span class="math inline">\(\mathbf{C} \in \mathbb{R}^{H}\)</span> ofthe <code>[CLS]</code> token from the last layer of the BERT model, theninput it into a fully connected layer <spanclass="math inline">\(\mathbf{W} \in \mathbb{R}^{H \times K}\)</span>,where <span class="math inline">\(K\)</span> is the number of labels,and calculated the corresponding standard classification loss <spanclass="math inline">\(\log(\text{softmax}(\mathbf{C}\mathbf{W}^{\top}))\)</span>.</p><h1 id="experimental-results">Experimental Results</h1><p>The dataset for this experiment consists of news headlines and theircorresponding category labels, including a development set and a testset. The development set contains 47,952 news items, and the test setcontains 15,986 news items. There are a total of 32 category labels,including finance, education, technology, sports, games, etc.</p><p>We further split the development set into a training set and avalidation set in an 8:2 ratio, and employed an early stopping strategy,stopping training when the accuracy on the validation set did notdecrease for three consecutive epochs. The remaining experimentalparameters are as follows: optimizer is Adam, learning rate is 5e-5,batch size is 64. The experiment was conducted on a Macbook Pro 2021,using the PyTorch deep learning framework.</p><p>To ensure the reliability of the results, for the same model, we ranthe experiment three times (with different random seeds) and reportedthe maximum and average performance of the model (the average value isin parentheses).</p><table><thead><tr><th style="text-align: center;">Model</th><th style="text-align: center;">Validation Set</th><th style="text-align: center;">Test Set</th></tr></thead><tbody><tr><td style="text-align: center;">BERT</td><td style="text-align: center;">0.8558 (0.8533)</td><td style="text-align: center;">0.8588 (0.8529)</td></tr><tr><td style="text-align: center;">BERT-wwm</td><td style="text-align: center;">0.8599 (0.8569)</td><td style="text-align: center;">0.8568 (0.8553)</td></tr><tr><td style="text-align: center;">BERT-wwm-ext</td><td style="text-align: center;">0.8608 (0.8592)</td><td style="text-align: center;"><strong>0.8636</strong>(<strong>0.8592</strong>)</td></tr><tr><td style="text-align: center;">RoBERTa-wwm-ext</td><td style="text-align: center;"><strong>0.8637</strong>(<strong>0.8604</strong>)</td><td style="text-align: center;">0.8608 (0.8588)</td></tr></tbody></table><p>The above experimental results indicate that the BERT-wwm-ext modelachieved the best performance on the test set, with an accuracy of86.36%. The RoBERTa-wwm-ext model performed best on the validation set,with an accuracy of 86.37%.</p><p>The experiment code has been open-sourced and is available at: <ahref="https://github.com/SignorinoY/bert-classification">https://github.com/SignorinoY/bert-classification</a>.</p><h1 class="unnumbered" id="bibliography">References</h1><div id="refs" class="references csl-bib-body hanging-indent"data-entry-spacing="0" data-line-spacing="2" role="list"><div id="ref-cui2021pre" class="csl-entry" role="listitem">Cui, Y., Che, W., Liu, T., Qin, B., &amp; Yang, Z. (2021). Pre-trainingwith whole word masking for chinese <span>BERT</span>. <em>IEEE/ACMTransactions on Audio, Speech and Language Processing</em>, <em>29</em>,3504–3514. <ahref="https://doi.org/10.1109/TASLP.2021.3124365">https://doi.org/10.1109/TASLP.2021.3124365</a></div><div id="ref-devlin2019bert" class="csl-entry" role="listitem">Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019).<span>BERT</span>: Pre-training of deep bidirectional transformers forlanguage understanding. In J. Burstein, C. Doran, &amp; T. Solorio(Eds.), <em>Proceedings of the 2019 <span>Conference</span> of the<span>North American Chapter</span> of the <span>Association</span> for<span>Computational Linguistics</span>: <span>Human LanguageTechnologies</span>, <span>Volume</span> 1 (<span>Long</span> and<span>Short Papers</span>)</em> (pp. 4171–4186). Association forComputational Linguistics. <ahref="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;This experiment report explores the process of using the open-source
BERT (Bidirectional Encoder Representations from Transformers) model for
Chinese text classification. By fine-tuning this model on a specific
news dataset, we evaluate its classification performance and accuracy.
The report systematically analyzes the basic principles, architectural
design, pre-training tasks, and fine-tuning methods of the BERT
pre-trained model while providing experimental results to assess the
model&#39;s performance.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Natural Language Processing" scheme="https://gongziyang.com/tags/Natural-Language-Processing/"/>
    
    <category term="Large Language Model" scheme="https://gongziyang.com/tags/Large-Language-Model/"/>
    
    <category term="BERT" scheme="https://gongziyang.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Review of Distributed Learning on Non/Semi-parametric Estimation</title>
    <link href="https://gongziyang.com/post/distributed-learning-non-semi-parametric-estimation/"/>
    <id>https://gongziyang.com/post/distributed-learning-non-semi-parametric-estimation/</id>
    <published>2024-05-31T01:38:09.000Z</published>
    <updated>2024-05-31T06:18:48.997Z</updated>
    
    <content type="html"><![CDATA[<p>This article provides a comprehensive review of distributed learningmethods for nonparametric and semiparametric estimation.</p><span id="more"></span><h1 id="divide-and-conquer-or-one-shot-methods">Divide-and-Conquer (orOne-Shot) Methods</h1><ul><li><span class="citation" data-cites="zhang2015divide">Zhang et al. (<ahref="#ref-zhang2015divide" role="doc-biblioref">2015</a>)</span>, <spanclass="citation" data-cites="lin2017distributed">Lin et al. (<ahref="#ref-lin2017distributed" role="doc-biblioref">2017</a>)</span>propose a method for nonparametric estimation by averaging local kernelridge regression estimators.</li><li><span class="citation" data-cites="zhao2016partially">Zhao et al.(<a href="#ref-zhao2016partially"role="doc-biblioref">2016</a>)</span></li><li><span class="citation" data-cites="lian2019projected">Lian et al.(<a href="#ref-lian2019projected" role="doc-biblioref">2019</a>)</span>(B-spline), <span class="citation" data-cites="wang2021distributed">Wanget al. (<a href="#ref-wang2021distributed"role="doc-biblioref">2021</a>)</span> (B-spline), <span class="citation"data-cites="lv2022debiased">Lv &amp; Lian (<a href="#ref-lv2022debiased"role="doc-biblioref">2022</a>)</span> (RKHS)</li><li><span class="citation" data-cites="chen2022distributed">Chen et al.(<a href="#ref-chen2022distributed"role="doc-biblioref">2022</a>)</span> investigate the use of thekernel-based Smoothed Maximum Score Estimator (SMSE) for solvingsemi-parametric binary response models.</li></ul><h1 id="communication-efficient-methods">Communication-EfficientMethods</h1><ul><li><span class="citation" data-cites="gao2023communicationa">Gao &amp;Wang (<a href="#ref-gao2023communicationa"role="doc-biblioref">2023</a>)</span> consider the partially linearmodel and propose a communication-efficient method based on the localpolynomial regression.</li><li><span class="citation" data-cites="chen2022distributed">Chen et al.(<a href="#ref-chen2022distributed"role="doc-biblioref">2022</a>)</span> also consider thecommunication-efficient distributed estimation of the partially linearmodel using the SMSE.</li></ul><h1 class="unnumbered" id="bibliography">References</h1><div id="refs" class="references csl-bib-body hanging-indent"data-entry-spacing="0" data-line-spacing="2" role="list"><div id="ref-chen2022distributed" class="csl-entry" role="listitem">Chen, X., Jing, W., Liu, W., &amp; Zhang, Y. (2022). <em>Distributedestimation and inference for semi-parametric binary response models</em>(arXiv:2210.08393). arXiv. <ahref="https://arxiv.org/abs/2210.08393">https://arxiv.org/abs/2210.08393</a></div><div id="ref-gao2023communicationa" class="csl-entry" role="listitem">Gao, J., &amp; Wang, L. (2023). Communication-efficient distributedestimation of partially linear additive models for large-scale data.<em>Information Sciences</em>, <em>631</em>, 185–201. <ahref="https://doi.org/10.1016/j.ins.2023.02.065">https://doi.org/10.1016/j.ins.2023.02.065</a></div><div id="ref-lian2019projected" class="csl-entry" role="listitem">Lian, H., Zhao, K., &amp; Lv, S. (2019). Projected spline estimation ofthe nonparametric function in high-dimensional partially linear modelsfor massive data. <em>Annals of Statistics</em>, <em>47</em>(5),2922–2949. <ahref="https://doi.org/10.1214/18-AOS1769">https://doi.org/10.1214/18-AOS1769</a></div><div id="ref-lin2017distributed" class="csl-entry" role="listitem">Lin, S.-B., Guo, X., &amp; Zhou, D.-X. (2017). Distributed learning withregularized least squares. <em>Journal of Machine LearningResearch</em>, <em>18</em>(92), 1–31.</div><div id="ref-lv2022debiased" class="csl-entry" role="listitem">Lv, S., &amp; Lian, H. (2022). Debiased distributed learning for sparsepartial linear models in high dimensions. <em>Journal of MachineLearning Research</em>, <em>23</em>(2), 1–32.</div><div id="ref-wang2021distributed" class="csl-entry" role="listitem">Wang, Y., Zhang, W., &amp; Lian, H. (2021). Distributed partially linearadditive models with a high dimensional linear part. <em>IEEETransactions on Signal and Information Processing over Networks</em>,<em>7</em>, 611–625. <ahref="https://doi.org/10.1109/TSIPN.2021.3111555">https://doi.org/10.1109/TSIPN.2021.3111555</a></div><div id="ref-zhang2015divide" class="csl-entry" role="listitem">Zhang, Y., Duchi, J., &amp; Wainwright, M. (2015). Divide and conquerkernel ridge regression: A distributed algorithm with minimax optimalrates. <em>Journal of Machine Learning Research</em>, <em>16</em>(1),3299–3340.</div><div id="ref-zhao2016partially" class="csl-entry" role="listitem">Zhao, T., Cheng, G., &amp; Liu, H. (2016). A partially linear frameworkfor massive heterogeneous data. <em>Annals of Statistics</em>,<em>44</em>(4). <ahref="https://doi.org/10.1214/15-AOS1410">https://doi.org/10.1214/15-AOS1410</a></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;This article provides a comprehensive review of distributed learning
methods for nonparametric and semiparametric estimation.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Cox Proportional Hazards Model</title>
    <link href="https://gongziyang.com/post/cox-proportional-hazards-model/"/>
    <id>https://gongziyang.com/post/cox-proportional-hazards-model/</id>
    <published>2024-01-02T13:34:32.000Z</published>
    <updated>2024-01-23T08:03:12.957Z</updated>
    
    <content type="html"><![CDATA[<p>The Cox proportional hazards model is a semi-parametric modelcommonly used in survival analysis. It is a regression model that aimsto model the hazard function of survival time <spanclass="math inline">\(T\)</span>, which represents the probability of anevent occurring at time <span class="math inline">\(t\)</span> giventhat the event has not occurred before time <spanclass="math inline">\(t\)</span>, that is, <span class="math display">\[\lambda(t)=\lim_{\Delta t\rightarrow 0}\frac{P(t\leq T&lt;t+\Deltat|T\geq t)}{\Delta t}.\]</span></p><span id="more"></span><h2 id="assumptions">Assumptions</h2><p>The Cox proportional hazards model assumes that the hazard functionfollows the form: <span class="math display">\[\lambda(t|X) = \lambda_0(t) \exp(X^T\beta),\]</span> where, <span class="math inline">\(\lambda_0(t)\)</span>represents the baseline hazard function, and <spanclass="math inline">\(\beta\)</span> is the vector of regressioncoefficients.</p><h2 id="estimation">Estimation</h2><p>The estimation of the Cox proportional hazards model is based on thepartial likelihood function. The partial likelihood function is definedas: <span class="math display">\[L(\beta) = \prod_{i=1}^{n}\left[\frac{\exp(X_i^T\beta)}{\sum_{j\inR(t_i)} \exp(X_j^T\beta)}\right]^{\delta_i},\]</span> where, <span class="math inline">\(R(t_i)\)</span> representsthe set of individuals at risk at time <spanclass="math inline">\(t_i\)</span>, and <spanclass="math inline">\(\delta_i\)</span> is an indicator variable thattakes the value 1 if the event occurs for individual <spanclass="math inline">\(i\)</span> and 0 otherwise.</p><p>The corresponding log partial likelihood function of <spanclass="math inline">\(\beta\)</span> is given by: <spanclass="math display">\[\ell(\beta)=\sum_{i=1}^{n}\delta_i\left[X_i^T\beta-\log\left(\sum_{j\inR(t_i)}\exp(X_j^T\beta)\right)\right]\]</span> and the cumulative baseline hazard function <spanclass="math inline">\(\Lambda_0(t)\)</span> is given by: <spanclass="math display">\[\Lambda_0(t)=\frac{\sum_{i=1}^{n}\exp(X_i^T\beta)I(t_i\leqt)}{\sum_{i=1}^{n} I(t_i\leq t)},\]</span> where, <span class="math inline">\(I(t_i\leq t)\)</span> is anindicator variable that takes the value 1 if <spanclass="math inline">\(t_i\leq t\)</span> and 0 otherwise.</p><details class="note default no-icon"><summary><p>Proof</p></summary><p>The likelihood function is defined as: <span class="math display">\[L(\beta)=\prod_{i=1}^{n}\lambda(t_i|X_i)^{\delta_i}S(t_i|X_i),\]</span> where <span class="math inline">\(S(t_i|X_i)\)</span>represents the survival function.</p><p>Assuming the baseline hazard function is a piecewise constantfunction, i.e., <spanclass="math inline">\(\lambda_0(t)=\lambda_k\)</span> for <spanclass="math inline">\(t\in[t_k,t_{k+1})\)</span>, the likelihoodfunction can be written as: <span class="math display">\[L(\beta)=\prod_{i=1}^{n}\lambda_0(t_i)\exp(X_i^T\beta)^{\delta_i}\exp\left(-\int_0^{t_i}\lambda_0(u)\exp(X_i^T\beta)\,\mathrm{d}u\right)\]</span></p></details>]]></content>
    
    
    <summary type="html">&lt;p&gt;The Cox proportional hazards model is a semi-parametric model
commonly used in survival analysis. It is a regression model that aims
to model the hazard function of survival time &lt;span
class=&quot;math inline&quot;&gt;&#92;(T&#92;)&lt;/span&gt;, which represents the probability of an
event occurring at time &lt;span class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; given
that the event has not occurred before time &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt;, that is, &lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;lambda(t)=&#92;lim_{&#92;Delta t&#92;rightarrow 0}&#92;frac{P(t&#92;leq T&amp;lt;t+&#92;Delta
t|T&#92;geq t)}{&#92;Delta t}.
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="Survival Analysis" scheme="https://gongziyang.com/tags/Survival-Analysis/"/>
    
  </entry>
  
</feed>
