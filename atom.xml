<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gong Ziyang</title>
  <icon>https://gongziyang.com/icon.png</icon>
  
  <link href="https://gongziyang.com/atom.xml" rel="self"/>
  
  <link href="https://gongziyang.com/"/>
  <updated>2024-06-13T14:27:54.245Z</updated>
  <id>https://gongziyang.com/</id>
  
  <author>
    <name>Gong Ziyang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>实验报告：BERT预训练模型</title>
    <link href="https://gongziyang.com/post/bert-pre-training-model-experimental-report/"/>
    <id>https://gongziyang.com/post/bert-pre-training-model-experimental-report/</id>
    <published>2024-06-10T10:00:00.000Z</published>
    <updated>2024-06-13T14:27:54.245Z</updated>
    
    <content type="html"><![CDATA[<p>本实验报告探讨了利用开源 BERT（Bidirectional Encoder RepresentationsfromTransformers）模型进行中文文本分类的过程。通过在特定新闻数据集上微调该模型，我们评估了其分类性能和准确性。本实验报告旨在系统性地分析 BERT预训练模型的基本原理、架构设计、预训练任务和微调方法，同时提供实验结果以评估模型表现。</p><span id="more"></span><h1 id="模型架构">模型架构</h1><p>BERT 是由 Google AI 团队于 2018 年提出的基于 Transformer的语言模型，通过预训练和微调显著提升了多项自然语言处理任务的性能 <spanclass="citation" data-cites="devlin2019bert">(<ahref="#ref-devlin2019bert" role="doc-biblioref">Devlin et al.,2019</a>)</span>。BERT模型的架构由多层自注意力层和前馈神经网络层构成。在本实验中，采用了BERT-base 模型架构，其包含 12 层、768 个隐藏单元、12个注意力头，总参数量达 1.1 亿。值得注意的是，BERT-base 与 OpenAI 的 GPT模型架构相似，但其训练过程采用了双向语言模型的目标。</p><h1 id="预训练任务">预训练任务</h1><p>为了更好地进行中文自然语言处理任务，本实验选择了以下几种 BERT预训练模型，其具体信息如下：</p><table><thead><tr><th style="text-align: center;">模型</th><th style="text-align: center;">数据集</th><th style="text-align: center;">Mask</th><th style="text-align: center;">预训练任务</th></tr></thead><tbody><tr><td style="text-align: center;"><code>BERT</code> <span class="citation"data-cites="devlin2019bert">(<a href="#ref-devlin2019bert"role="doc-biblioref">Devlin et al., 2019</a>)</span></td><td style="text-align: center;">wiki</td><td style="text-align: center;">Word Piece</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>BERT-wwm</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki</td><td style="text-align: center;">WWM</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>BERT-wwm-ext</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki+ext</td><td style="text-align: center;">WWM</td><td style="text-align: center;">MLM+NSP</td></tr><tr><td style="text-align: center;"><code>RoBERTa-wwm-ext</code> <spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></td><td style="text-align: center;">wiki+ext</td><td style="text-align: center;">WWM</td><td style="text-align: center;">DMLM</td></tr></tbody></table><ul><li><strong>数据集</strong>：<code>wiki</code>表示中文维基百科，<code>ext</code>表示其他百科、新闻、问答等数据集。</li><li><strong>Mask</strong>：<code>Word Piece</code>表示使用字粒度切分，忽略传统 NLP 中的中文分词（CWS）；<code>WWM</code>(Whole Word Masking) 使用哈工大 LTP作为分词工具，对组成同一个词的汉字全部进行 Mask 操作。<spanclass="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"role="doc-biblioref">Cui et al., 2021</a>)</span></li><li><strong>预训练任务</strong>： <code>MLM</code>（Masked LanguageModel）在输入中随机 Mask 一些 token，然后预测这些 Mask 的token；<code>NSP</code>（Next SentencePrediction）预测两个句子是否连续；<code>DMLM</code>（Dynamic MaskingLanguage Model）在输入中随机 Mask 一些 token，并动态调整 Mask的比例。</li></ul><h1 id="微调任务">微调任务</h1><p>在本实验的下游任务---文本分类中，我们采用了 <span class="citation"data-cites="devlin2019bert">Devlin et al. (<a href="#ref-devlin2019bert"role="doc-biblioref">2019</a>)</span>提出的方法。我们提取了BERT模型最后一层的 <code>[CLS]</code>标记的输出向量 <spanclass="math inline">\(\mathbf{C}\in\mathbb{R}^{H}\)</span>，然后将其输入到一个全连接层<span class="math inline">\(\mathbf{W}\in\mathbb{R}^{H\times K}\)</span>中，其中 <span class="math inline">\(K\)</span>为标签数目，并计算相应的标准分类损失 <spanclass="math inline">\(\log(\text{softmax}(\mathbf{C}\mathbf{W}^{\top}))\)</span>。</p><h1 id="实验结果">实验结果</h1><p>该实验数据集由新闻标题及其对应的类别标签组成，包含一个开发集和一个测试集。开发集有47,952 条新闻数据，测试集有 15,986 条新闻数据。类别标签共 32个，包括财经、教育、科技、体育、游戏等。</p><p>我们将开发集按照 8:2的比例再次划分为训练集和验证集，并采用早停策略，当验证集上的准确率连续 3次未下降时，停止训练。其余实验参数设置如下：优化器为 Adam，学习率为5e-5，批大小为 64。 该实验在 Macbook Pro 2021 上进行，使用 PyTorch深度学习框架。</p><p>为了保证结果的可靠性，对于同一模型，我们运行 3遍（不同随机种子），并汇报模型性能的最大值和平均值（括号内为平均值）。</p><table><thead><tr><th style="text-align: center;">模型</th><th style="text-align: center;">验证集</th><th style="text-align: center;">测试集</th></tr></thead><tbody><tr><td style="text-align: center;">BERT</td><td style="text-align: center;">0.8558 (0.8533)</td><td style="text-align: center;">0.8588 (0.8529)</td></tr><tr><td style="text-align: center;">BERT-wwm</td><td style="text-align: center;">0.8599 (0.8569)</td><td style="text-align: center;">0.8568 (0.8553)</td></tr><tr><td style="text-align: center;">BERT-wwm-ext</td><td style="text-align: center;">0.8608 (0.8592)</td><td style="text-align: center;"><strong>0.8636</strong>(<strong>0.8592</strong>)</td></tr><tr><td style="text-align: center;">RoBERTa-wwm-ext</td><td style="text-align: center;"><strong>0.8637</strong>(<strong>0.8604</strong>)</td><td style="text-align: center;">0.8608 (0.8588)</td></tr></tbody></table><p>上述实验结果表明，BERT-wwm-ext模型在测试集上取得了最佳性能，其准确率达到 86.36%。RoBERTa-wwm-ext模型在验证集上表现最佳，准确率达到 86.37%。</p><p>该实验代码已开源，详见：<ahref="https://github.com/SignorinoY/bert-classification">https://github.com/SignorinoY/bert-classification</a>。</p><h1 class="unnumbered" id="bibliography">References</h1><div id="refs" class="references csl-bib-body hanging-indent"data-entry-spacing="0" data-line-spacing="2" role="list"><div id="ref-cui2021pre" class="csl-entry" role="listitem">Cui, Y., Che, W., Liu, T., Qin, B., &amp; Yang, Z. (2021). Pre-trainingwith whole word masking for chinese <span>BERT</span>. <em>IEEE/ACMTransactions on Audio, Speech and Language Processing</em>, <em>29</em>,3504–3514. <ahref="https://doi.org/10.1109/TASLP.2021.3124365">https://doi.org/10.1109/TASLP.2021.3124365</a></div><div id="ref-devlin2019bert" class="csl-entry" role="listitem">Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019).<span>BERT</span>: Pre-training of deep bidirectional transformers forlanguage understanding. In J. Burstein, C. Doran, &amp; T. Solorio(Eds.), <em>Proceedings of the 2019 <span>Conference</span> of the<span>North American Chapter</span> of the <span>Association</span> for<span>Computational Linguistics</span>: <span>Human LanguageTechnologies</span>, <span>Volume</span> 1 (<span>Long</span> and<span>Short Papers</span>)</em> (pp. 4171–4186). Association forComputational Linguistics. <ahref="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;本实验报告探讨了利用开源 BERT（Bidirectional Encoder Representations
from
Transformers）模型进行中文文本分类的过程。通过在特定新闻数据集上微调该模型，我们评估了其分类性能和准确性。
本实验报告旨在系统性地分析 BERT
预训练模型的基本原理、架构设计、预训练任务和微调方法，同时提供实验结果以评估模型表现。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Natural Language Processing" scheme="https://gongziyang.com/tags/Natural-Language-Processing/"/>
    
    <category term="Large Language Model" scheme="https://gongziyang.com/tags/Large-Language-Model/"/>
    
    <category term="BERT" scheme="https://gongziyang.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Review of Distributed Learning on Non/Semi-parametric Estimation</title>
    <link href="https://gongziyang.com/post/distributed-learning-non-semi-parametric-estimation/"/>
    <id>https://gongziyang.com/post/distributed-learning-non-semi-parametric-estimation/</id>
    <published>2024-05-31T01:38:09.000Z</published>
    <updated>2024-05-31T06:18:48.997Z</updated>
    
    <content type="html"><![CDATA[<p>This article provides a comprehensive review of distributed learningmethods for nonparametric and semiparametric estimation.</p><span id="more"></span><h1 id="divide-and-conquer-or-one-shot-methods">Divide-and-Conquer (orOne-Shot) Methods</h1><ul><li><span class="citation" data-cites="zhang2015divide">Zhang et al. (<ahref="#ref-zhang2015divide" role="doc-biblioref">2015</a>)</span>, <spanclass="citation" data-cites="lin2017distributed">Lin et al. (<ahref="#ref-lin2017distributed" role="doc-biblioref">2017</a>)</span>propose a method for nonparametric estimation by averaging local kernelridge regression estimators.</li><li><span class="citation" data-cites="zhao2016partially">Zhao et al.(<a href="#ref-zhao2016partially"role="doc-biblioref">2016</a>)</span></li><li><span class="citation" data-cites="lian2019projected">Lian et al.(<a href="#ref-lian2019projected" role="doc-biblioref">2019</a>)</span>(B-spline), <span class="citation" data-cites="wang2021distributed">Wanget al. (<a href="#ref-wang2021distributed"role="doc-biblioref">2021</a>)</span> (B-spline), <span class="citation"data-cites="lv2022debiased">Lv &amp; Lian (<a href="#ref-lv2022debiased"role="doc-biblioref">2022</a>)</span> (RKHS)</li><li><span class="citation" data-cites="chen2022distributed">Chen et al.(<a href="#ref-chen2022distributed"role="doc-biblioref">2022</a>)</span> investigate the use of thekernel-based Smoothed Maximum Score Estimator (SMSE) for solvingsemi-parametric binary response models.</li></ul><h1 id="communication-efficient-methods">Communication-EfficientMethods</h1><ul><li><span class="citation" data-cites="gao2023communicationa">Gao &amp;Wang (<a href="#ref-gao2023communicationa"role="doc-biblioref">2023</a>)</span> consider the partially linearmodel and propose a communication-efficient method based on the localpolynomial regression.</li><li><span class="citation" data-cites="chen2022distributed">Chen et al.(<a href="#ref-chen2022distributed"role="doc-biblioref">2022</a>)</span> also consider thecommunication-efficient distributed estimation of the partially linearmodel using the SMSE.</li></ul><h1 class="unnumbered" id="bibliography">References</h1><div id="refs" class="references csl-bib-body hanging-indent"data-entry-spacing="0" data-line-spacing="2" role="list"><div id="ref-chen2022distributed" class="csl-entry" role="listitem">Chen, X., Jing, W., Liu, W., &amp; Zhang, Y. (2022). <em>Distributedestimation and inference for semi-parametric binary response models</em>(arXiv:2210.08393). arXiv. <ahref="https://arxiv.org/abs/2210.08393">https://arxiv.org/abs/2210.08393</a></div><div id="ref-gao2023communicationa" class="csl-entry" role="listitem">Gao, J., &amp; Wang, L. (2023). Communication-efficient distributedestimation of partially linear additive models for large-scale data.<em>Information Sciences</em>, <em>631</em>, 185–201. <ahref="https://doi.org/10.1016/j.ins.2023.02.065">https://doi.org/10.1016/j.ins.2023.02.065</a></div><div id="ref-lian2019projected" class="csl-entry" role="listitem">Lian, H., Zhao, K., &amp; Lv, S. (2019). Projected spline estimation ofthe nonparametric function in high-dimensional partially linear modelsfor massive data. <em>Annals of Statistics</em>, <em>47</em>(5),2922–2949. <ahref="https://doi.org/10.1214/18-AOS1769">https://doi.org/10.1214/18-AOS1769</a></div><div id="ref-lin2017distributed" class="csl-entry" role="listitem">Lin, S.-B., Guo, X., &amp; Zhou, D.-X. (2017). Distributed learning withregularized least squares. <em>Journal of Machine LearningResearch</em>, <em>18</em>(92), 1–31.</div><div id="ref-lv2022debiased" class="csl-entry" role="listitem">Lv, S., &amp; Lian, H. (2022). Debiased distributed learning for sparsepartial linear models in high dimensions. <em>Journal of MachineLearning Research</em>, <em>23</em>(2), 1–32.</div><div id="ref-wang2021distributed" class="csl-entry" role="listitem">Wang, Y., Zhang, W., &amp; Lian, H. (2021). Distributed partially linearadditive models with a high dimensional linear part. <em>IEEETransactions on Signal and Information Processing over Networks</em>,<em>7</em>, 611–625. <ahref="https://doi.org/10.1109/TSIPN.2021.3111555">https://doi.org/10.1109/TSIPN.2021.3111555</a></div><div id="ref-zhang2015divide" class="csl-entry" role="listitem">Zhang, Y., Duchi, J., &amp; Wainwright, M. (2015). Divide and conquerkernel ridge regression: A distributed algorithm with minimax optimalrates. <em>Journal of Machine Learning Research</em>, <em>16</em>(1),3299–3340.</div><div id="ref-zhao2016partially" class="csl-entry" role="listitem">Zhao, T., Cheng, G., &amp; Liu, H. (2016). A partially linear frameworkfor massive heterogeneous data. <em>Annals of Statistics</em>,<em>44</em>(4). <ahref="https://doi.org/10.1214/15-AOS1410">https://doi.org/10.1214/15-AOS1410</a></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;This article provides a comprehensive review of distributed learning
methods for nonparametric and semiparametric estimation.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Cox Proportional Hazards Model</title>
    <link href="https://gongziyang.com/post/cox-proportional-hazards-model/"/>
    <id>https://gongziyang.com/post/cox-proportional-hazards-model/</id>
    <published>2024-01-02T13:34:32.000Z</published>
    <updated>2024-01-23T08:03:12.957Z</updated>
    
    <content type="html"><![CDATA[<p>The Cox proportional hazards model is a semi-parametric modelcommonly used in survival analysis. It is a regression model that aimsto model the hazard function of survival time <spanclass="math inline">\(T\)</span>, which represents the probability of anevent occurring at time <span class="math inline">\(t\)</span> giventhat the event has not occurred before time <spanclass="math inline">\(t\)</span>, that is, <span class="math display">\[\lambda(t)=\lim_{\Delta t\rightarrow 0}\frac{P(t\leq T&lt;t+\Deltat|T\geq t)}{\Delta t}.\]</span></p><span id="more"></span><h2 id="assumptions">Assumptions</h2><p>The Cox proportional hazards model assumes that the hazard functionfollows the form: <span class="math display">\[\lambda(t|X) = \lambda_0(t) \exp(X^T\beta),\]</span> where, <span class="math inline">\(\lambda_0(t)\)</span>represents the baseline hazard function, and <spanclass="math inline">\(\beta\)</span> is the vector of regressioncoefficients.</p><h2 id="estimation">Estimation</h2><p>The estimation of the Cox proportional hazards model is based on thepartial likelihood function. The partial likelihood function is definedas: <span class="math display">\[L(\beta) = \prod_{i=1}^{n}\left[\frac{\exp(X_i^T\beta)}{\sum_{j\inR(t_i)} \exp(X_j^T\beta)}\right]^{\delta_i},\]</span> where, <span class="math inline">\(R(t_i)\)</span> representsthe set of individuals at risk at time <spanclass="math inline">\(t_i\)</span>, and <spanclass="math inline">\(\delta_i\)</span> is an indicator variable thattakes the value 1 if the event occurs for individual <spanclass="math inline">\(i\)</span> and 0 otherwise.</p><p>The corresponding log partial likelihood function of <spanclass="math inline">\(\beta\)</span> is given by: <spanclass="math display">\[\ell(\beta)=\sum_{i=1}^{n}\delta_i\left[X_i^T\beta-\log\left(\sum_{j\inR(t_i)}\exp(X_j^T\beta)\right)\right]\]</span> and the cumulative baseline hazard function <spanclass="math inline">\(\Lambda_0(t)\)</span> is given by: <spanclass="math display">\[\Lambda_0(t)=\frac{\sum_{i=1}^{n}\exp(X_i^T\beta)I(t_i\leqt)}{\sum_{i=1}^{n} I(t_i\leq t)},\]</span> where, <span class="math inline">\(I(t_i\leq t)\)</span> is anindicator variable that takes the value 1 if <spanclass="math inline">\(t_i\leq t\)</span> and 0 otherwise.</p><details class="note default no-icon"><summary><p>Proof</p></summary><p>The likelihood function is defined as: <span class="math display">\[L(\beta)=\prod_{i=1}^{n}\lambda(t_i|X_i)^{\delta_i}S(t_i|X_i),\]</span> where <span class="math inline">\(S(t_i|X_i)\)</span>represents the survival function.</p><p>Assuming the baseline hazard function is a piecewise constantfunction, i.e., <spanclass="math inline">\(\lambda_0(t)=\lambda_k\)</span> for <spanclass="math inline">\(t\in[t_k,t_{k+1})\)</span>, the likelihoodfunction can be written as: <span class="math display">\[L(\beta)=\prod_{i=1}^{n}\lambda_0(t_i)\exp(X_i^T\beta)^{\delta_i}\exp\left(-\int_0^{t_i}\lambda_0(u)\exp(X_i^T\beta)\,\mathrm{d}u\right)\]</span></p></details>]]></content>
    
    
    <summary type="html">&lt;p&gt;The Cox proportional hazards model is a semi-parametric model
commonly used in survival analysis. It is a regression model that aims
to model the hazard function of survival time &lt;span
class=&quot;math inline&quot;&gt;&#92;(T&#92;)&lt;/span&gt;, which represents the probability of an
event occurring at time &lt;span class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; given
that the event has not occurred before time &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt;, that is, &lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;lambda(t)=&#92;lim_{&#92;Delta t&#92;rightarrow 0}&#92;frac{P(t&#92;leq T&amp;lt;t+&#92;Delta
t|T&#92;geq t)}{&#92;Delta t}.
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="Survival Analysis" scheme="https://gongziyang.com/tags/Survival-Analysis/"/>
    
  </entry>
  
</feed>
