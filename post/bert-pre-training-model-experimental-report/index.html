<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="DudtdKuiM8ewkC5--j3YR4aaM7wz73ZwZfcJ1LEX0fk">
  <meta name="msvalidate.01" content="743D41A43C9BC54DAA25F5848606702E">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"gongziyang.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="This experiment report explores the process of using the open-source BERT (Bidirectional Encoder Representations from Transformers) model for Chinese text classification. By fine-tuning this model on">
<meta property="og:type" content="article">
<meta property="og:title" content="Experiment Report: BERT Pre-trained Model">
<meta property="og:url" content="https://gongziyang.com/post/bert-pre-training-model-experimental-report/index.html">
<meta property="og:site_name" content="Gong Ziyang">
<meta property="og:description" content="This experiment report explores the process of using the open-source BERT (Bidirectional Encoder Representations from Transformers) model for Chinese text classification. By fine-tuning this model on">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-06-10T10:00:00.000Z">
<meta property="article:modified_time" content="2024-06-26T07:22:41.407Z">
<meta property="article:author" content="Gong Ziyang">
<meta property="article:tag" content="Natural Language Processing">
<meta property="article:tag" content="Large Language Model">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://gongziyang.com/post/bert-pre-training-model-experimental-report/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://gongziyang.com/post/bert-pre-training-model-experimental-report/","path":"post/bert-pre-training-model-experimental-report/","title":"Experiment Report: BERT Pre-trained Model"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Experiment Report: BERT Pre-trained Model | Gong Ziyang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Gong Ziyang" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Gong Ziyang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-cv"><a href="/cv/" rel="section"><i class="fa fa-id-card fa-fw"></i>CV</a></li><li class="menu-item menu-item-publications"><a href="/publications/" rel="section"><i class="fa fa-book fa-fw"></i>Publications</a></li><li class="menu-item menu-item-blog"><a href="/blog/" rel="section"><i class="fa fa-pencil fa-fw"></i>Blog</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#model-architecture"><span class="nav-number">1.</span> <span class="nav-text">Model Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pre-training-tasks"><span class="nav-number">2.</span> <span class="nav-text">Pre-training Tasks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#fine-tuning-tasks"><span class="nav-number">3.</span> <span class="nav-text">Fine-tuning Tasks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#experimental-results"><span class="nav-number">4.</span> <span class="nav-text">Experimental Results</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bibliography"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gong Ziyang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Gong Ziyang</p>
  <div class="site-description" itemprop="description">PhD candidate in Statistics</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/signorinoy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;signorinoy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:gongziyang@swufe.edu.cn" title="E-Mail → mailto:gongziyang@swufe.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://gongziyang.com/post/bert-pre-training-model-experimental-report/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Gong Ziyang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gong Ziyang">
      <meta itemprop="description" content="PhD candidate in Statistics">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Experiment Report: BERT Pre-trained Model | Gong Ziyang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Experiment Report: BERT Pre-trained Model
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-10 18:00:00" itemprop="dateCreated datePublished" datetime="2024-06-10T18:00:00+08:00">2024-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-06-26 15:22:41" itemprop="dateModified" datetime="2024-06-26T15:22:41+08:00">2024-06-26</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>This experiment report explores the process of using the open-source
BERT (Bidirectional Encoder Representations from Transformers) model for
Chinese text classification. By fine-tuning this model on a specific
news dataset, we evaluate its classification performance and accuracy.
The report systematically analyzes the basic principles, architectural
design, pre-training tasks, and fine-tuning methods of the BERT
pre-trained model while providing experimental results to assess the
model's performance.</p>
<span id="more"></span>
<h1 id="model-architecture">Model Architecture</h1>
<p>BERT is a language model based on Transformers proposed by the Google
AI team in 2018, which significantly improved the performance of various
natural language processing tasks through pre-training and fine-tuning
<span class="citation" data-cites="devlin2019bert">(<a
href="#ref-devlin2019bert" role="doc-biblioref">Devlin et al.,
2019</a>)</span>. The architecture of the BERT model consists of
multiple layers of self-attention and feedforward neural network layers.
In this experiment, we adopted the BERT-base model architecture, which
includes 12 layers, 768 hidden units, 12 attention heads, and a total of
110 million parameters. It is noteworthy that the BERT-base model is
similar to the architecture of OpenAI's GPT model but uses bidirectional
language model objectives during training.</p>
<h1 id="pre-training-tasks">Pre-training Tasks</h1>
<p>To better perform Chinese natural language processing tasks, we
selected the following BERT pre-trained models for this experiment, with
specific information as follows:</p>
<table>
<colgroup>
<col style="width: 34%" />
<col style="width: 14%" />
<col style="width: 19%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Mask</th>
<th style="text-align: center;">Pre-training Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><code>BERT</code> <span class="citation"
data-cites="devlin2019bert">(<a href="#ref-devlin2019bert"
role="doc-biblioref">Devlin et al., 2019</a>)</span></td>
<td style="text-align: center;">wiki</td>
<td style="text-align: center;">Word Piece</td>
<td style="text-align: center;">MLM+NSP</td>
</tr>
<tr>
<td style="text-align: center;"><code>BERT-wwm</code> <span
class="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"
role="doc-biblioref">Cui et al., 2021</a>)</span></td>
<td style="text-align: center;">wiki</td>
<td style="text-align: center;">WWM</td>
<td style="text-align: center;">MLM+NSP</td>
</tr>
<tr>
<td style="text-align: center;"><code>BERT-wwm-ext</code> <span
class="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"
role="doc-biblioref">Cui et al., 2021</a>)</span></td>
<td style="text-align: center;">wiki+ext</td>
<td style="text-align: center;">WWM</td>
<td style="text-align: center;">MLM+NSP</td>
</tr>
<tr>
<td style="text-align: center;"><code>RoBERTa-wwm-ext</code> <span
class="citation" data-cites="cui2021pre">(<a href="#ref-cui2021pre"
role="doc-biblioref">Cui et al., 2021</a>)</span></td>
<td style="text-align: center;">wiki+ext</td>
<td style="text-align: center;">WWM</td>
<td style="text-align: center;">DMLM</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Dataset</strong>: <code>wiki</code> represents Chinese
Wikipedia, <code>ext</code> represents other encyclopedias, news, QA
datasets, etc.</li>
<li><strong>Mask</strong>: <code>Word Piece</code> indicates the use of
subword tokenization, ignoring traditional Chinese word segmentation
(CWS); <code>WWM</code> (Whole Word Masking) uses the LTP tool from HIT
to mask all characters in the same word. <span class="citation"
data-cites="cui2021pre">(<a href="#ref-cui2021pre"
role="doc-biblioref">Cui et al., 2021</a>)</span></li>
<li><strong>Pre-training Tasks</strong>: <code>MLM</code> (Masked
Language Model) randomly masks some tokens in the input and then
predicts these masked tokens; <code>NSP</code> (Next Sentence
Prediction) predicts whether two sentences are consecutive;
<code>DMLM</code> (Dynamic Masking Language Model) randomly masks some
tokens in the input and dynamically adjusts the masking ratio.</li>
</ul>
<h1 id="fine-tuning-tasks">Fine-tuning Tasks</h1>
<p>In the downstream task of text classification in this experiment, we
adopted the method proposed by <span class="citation"
data-cites="devlin2019bert">Devlin et al. (<a href="#ref-devlin2019bert"
role="doc-biblioref">2019</a>)</span>. We extracted the output vector
<span class="math inline">\(\mathbf{C} \in \mathbb{R}^{H}\)</span> of
the <code>[CLS]</code> token from the last layer of the BERT model, then
input it into a fully connected layer <span
class="math inline">\(\mathbf{W} \in \mathbb{R}^{H \times K}\)</span>,
where <span class="math inline">\(K\)</span> is the number of labels,
and calculated the corresponding standard classification loss <span
class="math inline">\(\log(\text{softmax}(\mathbf{C}\mathbf{W}^{\top}))\)</span>.</p>
<h1 id="experimental-results">Experimental Results</h1>
<p>The dataset for this experiment consists of news headlines and their
corresponding category labels, including a development set and a test
set. The development set contains 47,952 news items, and the test set
contains 15,986 news items. There are a total of 32 category labels,
including finance, education, technology, sports, games, etc.</p>
<p>We further split the development set into a training set and a
validation set in an 8:2 ratio, and employed an early stopping strategy,
stopping training when the accuracy on the validation set did not
decrease for three consecutive epochs. The remaining experimental
parameters are as follows: optimizer is Adam, learning rate is 5e-5,
batch size is 64. The experiment was conducted on a Macbook Pro 2021,
using the PyTorch deep learning framework.</p>
<p>To ensure the reliability of the results, for the same model, we ran
the experiment three times (with different random seeds) and reported
the maximum and average performance of the model (the average value is
in parentheses).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Validation Set</th>
<th style="text-align: center;">Test Set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.8558 (0.8533)</td>
<td style="text-align: center;">0.8588 (0.8529)</td>
</tr>
<tr>
<td style="text-align: center;">BERT-wwm</td>
<td style="text-align: center;">0.8599 (0.8569)</td>
<td style="text-align: center;">0.8568 (0.8553)</td>
</tr>
<tr>
<td style="text-align: center;">BERT-wwm-ext</td>
<td style="text-align: center;">0.8608 (0.8592)</td>
<td style="text-align: center;"><strong>0.8636</strong>
(<strong>0.8592</strong>)</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-wwm-ext</td>
<td style="text-align: center;"><strong>0.8637</strong>
(<strong>0.8604</strong>)</td>
<td style="text-align: center;">0.8608 (0.8588)</td>
</tr>
</tbody>
</table>
<p>The above experimental results indicate that the BERT-wwm-ext model
achieved the best performance on the test set, with an accuracy of
86.36%. The RoBERTa-wwm-ext model performed best on the validation set,
with an accuracy of 86.37%.</p>
<p>The experiment code has been open-sourced and is available at: <a
target="_blank" rel="noopener" href="https://github.com/SignorinoY/bert-classification">https://github.com/SignorinoY/bert-classification</a>.</p>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-cui2021pre" class="csl-entry" role="listitem">
Cui, Y., Che, W., Liu, T., Qin, B., &amp; Yang, Z. (2021). Pre-training
with whole word masking for chinese <span>BERT</span>. <em>IEEE/ACM
Transactions on Audio, Speech and Language Processing</em>, <em>29</em>,
3504–3514. <a
target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2021.3124365">https://doi.org/10.1109/TASLP.2021.3124365</a>
</div>
<div id="ref-devlin2019bert" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019).
<span>BERT</span>: Pre-training of deep bidirectional transformers for
language understanding. In J. Burstein, C. Doran, &amp; T. Solorio
(Eds.), <em>Proceedings of the 2019 <span>Conference</span> of the
<span>North American Chapter</span> of the <span>Association</span> for
<span>Computational Linguistics</span>: <span>Human Language
Technologies</span>, <span>Volume</span> 1 (<span>Long</span> and
<span>Short Papers</span>)</em> (pp. 4171–4186). Association for
Computational Linguistics. <a
target="_blank" rel="noopener" href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>
</div>
</div>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Gong Ziyang
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://gongziyang.com/post/bert-pre-training-model-experimental-report/" title="Experiment Report: BERT Pre-trained Model">https://gongziyang.com/post/bert-pre-training-model-experimental-report/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Natural-Language-Processing/" rel="tag"># Natural Language Processing</a>
              <a href="/tags/Large-Language-Model/" rel="tag"># Large Language Model</a>
              <a href="/tags/BERT/" rel="tag"># BERT</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/distributed-learning-non-semi-parametric-estimation/" rel="prev" title="Review of Distributed Learning on Non/Semi-parametric Estimation">
                  <i class="fa fa-angle-left"></i> Review of Distributed Learning on Non/Semi-parametric Estimation
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 1998 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Gong Ziyang</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"SignorinoY","repo":"SignorinoY.github.io","client_id":"2487e2583974d4a72a01","client_secret":"b83ec9c3984c1ae9f021d489c254238ba113ea68","admin_user":"SignorinoY","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"f2159cf7f7512842acccf14fb14d6be4"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
