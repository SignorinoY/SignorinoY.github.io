@misc{chen2022distributed,
  title = {Distributed Estimation and Inference for Semi-Parametric Binary Response Models},
  author = {Chen, Xi and Jing, Wenbo and Liu, Weidong and Zhang, Yichen},
  year = {2022},
  month = nov,
  number = {arXiv:2210.08393},
  eprint = {2210.08393},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2023-07-25},
  abstract = {The development of modern technology has enabled data collection of unprecedented size, which poses new challenges to many statistical estimation and inference problems. This paper studies the maximum score estimator of a semi-parametric binary choice model under a distributed computing environment without pre-specifying the noise distribution. An intuitive divide-and-conquer estimator is computationally expensive and restricted by a non-regular constraint on the number of machines, due to the highly non-smooth nature of the objective function. We propose (1) a one-shot divide-and-conquer estimator after smoothing the objective to relax the constraint, and (2) a multi-round estimator to completely remove the constraint via iterative smoothing. We specify an adaptive choice of kernel smoother with a sequentially shrinking bandwidth to achieve the superlinear improvement of the optimization error over the multiple iterations. The improved statistical accuracy per iteration is derived, and a quadratic convergence up to the optimal statistical error rate is established. We further provide two generalizations to handle the heterogeneity of datasets with covariate shift and high-dimensional problems where the parameter of interest is sparse.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Chen, 2022) Distributed estimation and inference for semi-parametric binary response models.pdf}
}

@article{lian2019projected,
  title = {Projected Spline Estimation of the Nonparametric Function in High-Dimensional Partially Linear Models for Massive Data},
  author = {Lian, Heng and Zhao, Kaifeng and Lv, Shaogao},
  year = {2019},
  month = oct,
  journal = {Annals of Statistics},
  volume = {47},
  number = {5},
  pages = {2922--2949},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/18-AOS1769},
  urldate = {2024-05-13},
  abstract = {In this paper, we consider the local asymptotics of the nonparametric function in a partially linear model, within the framework of the divide-and-conquer estimation. Unlike the fixed-dimensional setting in which the parametric part does not affect the nonparametric part, the high-dimensional setting makes the issue more complicated. In particular, when a sparsity-inducing penalty such as lasso is used to make the estimation of the linear part feasible, the bias introduced will propagate to the nonparametric part. We propose a novel approach for estimation of the nonparametric function and establish the local asymptotics of the estimator. The result is useful for massive data with possibly different linear coefficients in each subpopulation but common nonparametric function. Some numerical illustrations are also presented.},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Lian, 2019) Projected spline estimation of the nonparametric function in high-dimensional partially linear models for massive data.pdf}
}

@article{lin2017distributed,
  title = {Distributed Learning with Regularized Least Squares},
  author = {Lin, Shao-Bo and Guo, Xin and Zhou, Ding-Xuan},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {92},
  pages = {1--31},
  issn = {1533-7928},
  urldate = {2024-05-31},
  abstract = {We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the L2 L 2 -metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature.},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Lin, 2017) Distributed learning with regularized least squares.pdf}
}

@article{lv2022debiased,
  title = {Debiased Distributed Learning for Sparse Partial Linear Models in High Dimensions},
  author = {Lv, Shaogao and Lian, Heng},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {2},
  pages = {1--32},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Lv, 2022) Debiased distributed learning for sparse partial linear models in high dimensions.pdf}
}

@article{zhang2015divide,
  title = {Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates},
  shorttitle = {Divide and Conquer Kernel Ridge Regression},
  author = {Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  year = {2015},
  month = jan,
  journal = {Journal of Machine Learning Research},
  volume = {16},
  number = {1},
  pages = {3299--3340},
  issn = {1532-4435},
  abstract = {We study a decomposition-based scalable approach to kernel ridge regression, and show that it achieves minimax optimal convergence rates under relatively mild conditions. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset using a careful choice of the regularization parameter, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of subsets m may grow nearly linearly for finite-rank or Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Zhang, 2015) Divide and conquer kernel ridge regression a distributed algorithm with minimax optimal rates.pdf}
}

@article{zhao2016partially,
  title = {A Partially Linear Framework for Massive Heterogeneous Data},
  author = {Zhao, Tianqi and Cheng, Guang and Liu, Han},
  year = {2016},
  month = aug,
  journal = {Annals of Statistics},
  volume = {44},
  number = {4},
  publisher = {NIH Public Access},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1410},
  langid = {english},
  file = {/Users/gongziyang/Library/Mobile Documents/com~apple~CloudDocs/论文/(Zhao, 2016) A partially linear framework for massive heterogeneous data.pdf}
}
